{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound Emotion Detection System\n",
    "\n",
    "This notebook trains a deep learning model to detect emotions from audio files using the TESS Toronto emotional speech dataset.\n",
    "\n",
    "## Dataset Structure:\n",
    "- **OAF_***: Older actress female recordings\n",
    "- **YAF_***: Younger actress female recordings\n",
    "- **Emotions**: angry, disgust, fear, happy, neutral, sad, pleasant_surprise\n",
    "\n",
    "## Features:\n",
    "- **MFCCs**: Mel-frequency cepstral coefficients\n",
    "- **Chroma**: Pitch-related features\n",
    "- **Spectral Contrast**: Frequency band differences\n",
    "- **Zero Crossing Rate**: Signal changes\n",
    "- **Spectral Rolloff**: Frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path and parameters\n",
    "data_dir = \"../data/sound_data/TESS Toronto emotional speech set data\"\n",
    "sample_rate = 22050  # Standard audio sample rate\n",
    "duration = 3  # Duration in seconds (will pad/crop to this)\n",
    "n_mfcc = 40  # Number of MFCC features\n",
    "n_fft = 2048  # FFT window size\n",
    "hop_length = 512  # Hop length for STFT\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Sample rate: {sample_rate} Hz\")\n",
    "print(f\"Duration: {duration} seconds\")\n",
    "print(f\"MFCC features: {n_mfcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset structure\n",
    "emotions = []\n",
    "file_counts = {}\n",
    "\n",
    "for folder in os.listdir(data_dir):\n",
    "    if os.path.isdir(os.path.join(data_dir, folder)):\n",
    "        # Extract emotion from folder name\n",
    "        if '_' in folder:\n",
    "            emotion = folder.split('_')[-1]\n",
    "            if emotion not in emotions:\n",
    "                emotions.append(emotion)\n",
    "        \n",
    "        # Count files\n",
    "        file_path = os.path.join(data_dir, folder)\n",
    "        file_count = len([f for f in os.listdir(file_path) if f.endswith('.wav')])\n",
    "        file_counts[folder] = file_count\n",
    "\n",
    "print(\"Dataset Structure:\")\n",
    "for folder, count in file_counts.items():\n",
    "    print(f\"  {folder}: {count} files\")\n",
    "\n",
    "print(f\"\\nEmotions found: {emotions}\")\n",
    "print(f\"Total files: {sum(file_counts.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(file_path, max_pad_length=173):\n",
    "    \"\"\"\n",
    "    Extract comprehensive audio features from audio file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(file_path, duration=duration, sr=sample_rate)\n",
    "        \n",
    "        # Ensure consistent length\n",
    "        if len(y) < max_pad_length:\n",
    "            y = np.pad(y, (0, max_pad_length - len(y)), mode='constant')\n",
    "        else:\n",
    "            y = y[:max_pad_length]\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # 1. MFCCs (most important for speech emotion)\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        features['mfcc'] = mfccs\n",
    "        \n",
    "        # 2. Chroma features (pitch-related)\n",
    "        chroma = librosa.feature.chroma(y=y, sr=sr, hop_length=hop_length)\n",
    "        features['chroma'] = chroma\n",
    "        \n",
    "        # 3. Spectral Contrast\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=hop_length)\n",
    "        features['spectral_contrast'] = spectral_contrast\n",
    "        \n",
    "        # 4. Zero Crossing Rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(y, hop_length=hop_length)\n",
    "        features['zcr'] = zcr\n",
    "        \n",
    "        # 5. Spectral Rolloff\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, hop_length=hop_length)\n",
    "        features['spectral_rolloff'] = spectral_rolloff\n",
    "        \n",
    "        # 6. RMS Energy\n",
    "        rms = librosa.feature.rms(y=y, hop_length=hop_length)\n",
    "        features['rms'] = rms\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature extraction on a sample file\n",
    "sample_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
    "sample_folder = os.path.join(data_dir, sample_folders[0])\n",
    "sample_files = [f for f in os.listdir(sample_folder) if f.endswith('.wav')]\n",
    "sample_file = os.path.join(sample_folder, sample_files[0])\n",
    "\n",
    "print(f\"Sample file: {sample_file}\")\n",
    "\n",
    "# Extract features\n",
    "features = extract_audio_features(sample_file)\n",
    "\n",
    "if features:\n",
    "    print(\"\\nFeature shapes:\")\n",
    "    for name, feature in features.items():\n",
    "        print(f\"  {name}: {feature.shape}\")\n",
    "    \n",
    "    # Visualize MFCCs\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 3, 1)\n",
    "    librosa.display.specshow(features['mfcc'], sr=sample_rate, hop_length=hop_length, x_axis='time', cmap='coolwarm')\n",
    "    plt.colorbar()\n",
    "    plt.title('MFCCs')\n",
    "    \n",
    "    plt.subplot(2, 3, 2)\n",
    "    librosa.display.specshow(features['chroma'], sr=sample_rate, hop_length=hop_length, x_axis='time', y_axis='chroma', cmap='coolwarm')\n",
    "    plt.colorbar()\n",
    "    plt.title('Chroma')\n",
    "    \n",
    "    plt.subplot(2, 3, 3)\n",
    "    librosa.display.specshow(features['spectral_contrast'], sr=sample_rate, hop_length=hop_length, x_axis='time', cmap='coolwarm')\n",
    "    plt.colorbar()\n",
    "    plt.title('Spectral Contrast')\n",
    "    \n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(features['zcr'][0])\n",
    "    plt.title('Zero Crossing Rate')\n",
    "    \n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(features['spectral_rolloff'][0])\n",
    "    plt.title('Spectral Rolloff')\n",
    "    \n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(features['rms'][0])\n",
    "    plt.title('RMS Energy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Failed to extract features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process entire dataset\n",
    "X = []\n",
    "y = []\n",
    "file_paths = []\n",
    "processed_files = 0\n",
    "\n",
    "print(\"Processing audio files...\")\n",
    "\n",
    "for folder in os.listdir(data_dir):\n",
    "    if os.path.isdir(os.path.join(data_dir, folder)):\n",
    "        # Extract emotion label\n",
    "        if '_' in folder:\n",
    "            emotion = folder.split('_')[-1]\n",
    "            # Normalize emotion names\n",
    "            if emotion == 'pleasant_surprise':\n",
    "                emotion = 'surprise'\n",
    "            elif emotion == 'pleasant_surprised':\n",
    "                emotion = 'surprise'\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        folder_path = os.path.join(data_dir, folder)\n",
    "        \n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                \n",
    "                # Extract features\n",
    "                features = extract_audio_features(file_path)\n",
    "                \n",
    "                if features is not None:\n",
    "                    # Combine all features\n",
    "                    combined_features = np.concatenate([\n",
    "                        features['mfcc'],\n",
    "                        features['chroma'],\n",
    "                        features['spectral_contrast'],\n",
    "                        features['zcr'],\n",
    "                        features['spectral_rolloff'],\n",
    "                        features['rms']\n",
    "                    ], axis=0)\n",
    "                    \n",
    "                    X.append(combined_features)\n",
    "                    y.append(emotion)\n",
    "                    file_paths.append(file_path)\n",
    "                    processed_files += 1\n",
    "                    \n",
    "                    if processed_files % 100 == 0:\n",
    "                        print(f\"Processed {processed_files} files...\")\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Total files processed: {processed_files}\")\n",
    "print(f\"Feature shape: {np.array(X).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"Final dataset shape:\")\n",
    "print(f\"  X (features): {X.shape}\")\n",
    "print(f\"  y (labels): {y.shape}\")\n",
    "\n",
    "# Check emotion distribution\n",
    "unique_emotions, counts = np.unique(y, return_counts=True)\n",
    "print(f\"\\nEmotion distribution:\")\n",
    "for emotion, count in zip(unique_emotions, counts):\n",
    "    print(f\"  {emotion}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "n_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Encoded labels:\")\n",
    "for i, emotion in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {i}: {emotion}\")\n",
    "\n",
    "print(f\"\\nNumber of classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Features normalized\")\n",
    "print(f\"Normalized shape: {X_normalized.shape}\")\n",
    "\n",
    "# Save scaler for later use\n",
    "joblib.dump(scaler, '../model/sound_emotion_scaler.pkl')\n",
    "print(\"Scaler saved to ../model/sound_emotion_scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape features for CNN/LSTM input\n",
    "# Current shape: (n_samples, n_features)\n",
    "# We need: (n_samples, time_steps, n_features_per_step)\n",
    "\n",
    "# Calculate time steps and features per step\n",
    "total_features = X_normalized.shape[1]\n",
    "n_time_steps = 173  # Based on MFCC time frames\n",
    "n_features_per_step = total_features // n_time_steps\n",
    "\n",
    "# Reshape for sequential models\n",
    "X_reshaped = X_normalized.reshape(X_normalized.shape[0], n_time_steps, n_features_per_step)\n",
    "\n",
    "print(f\"Reshaped for sequential processing:\")\n",
    "print(f\"  Original shape: {X_normalized.shape}\")\n",
    "print(f\"  Reshaped: {X_reshaped.shape}\")\n",
    "print(f\"  Time steps: {n_time_steps}\")\n",
    "print(f\"  Features per step: {n_features_per_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reshaped, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Dataset split:\")\n",
    "print(f\"  Training: {X_train.shape} ({len(X_train)} samples)\")\n",
    "print(f\"  Validation: {X_val.shape} ({len(X_val)} samples)\")\n",
    "print(f\"  Testing: {X_test.shape} ({len(X_test)} samples)\")\n",
    "\n",
    "# Convert to categorical\n",
    "y_train_cat = to_categorical(y_train, num_classes=n_classes)\n",
    "y_val_cat = to_categorical(y_val, num_classes=n_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build advanced sound emotion detection model\n",
    "def build_sound_emotion_model(input_shape, n_classes):\n",
    "    model = Sequential([\n",
    "        # CNN layers for feature extraction\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv1D(128, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Conv1D(256, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # LSTM layers for temporal patterns\n",
    "        LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3),\n",
    "        LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
    "        \n",
    "        # Dense layers for classification\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        Dense(n_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "input_shape = (n_time_steps, n_features_per_step)\n",
    "model = build_sound_emotion_model(input_shape, n_classes)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../model/best_sound_emotion_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "print(f\"Training parameters:\")\n",
    "print(f\"  Epochs: {epochs}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\nðŸŽµ Training Sound Emotion Detection Model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"\\nðŸ“Š Evaluating Model Performance...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test set evaluation\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X_test, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = y_test\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "class_names = label_encoder.classes_\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix - Sound Emotion Detection')\n",
    "plt.ylabel('True Emotion')\n",
    "plt.xlabel('Predicted Emotion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax2.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"  Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"  Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model and components\n",
    "model.save('../model/sound_emotion_detector.keras')\n",
    "print(\"âœ… Model saved as: ../model/sound_emotion_detector.keras\")\n",
    "\n",
    "# Save label encoder\n",
    "joblib.dump(label_encoder, '../model/sound_emotion_label_encoder.pkl')\n",
    "print(\"âœ… Label encoder saved as: ../model/sound_emotion_label_encoder.pkl\")\n",
    "\n",
    "# Save model architecture\n",
    "with open('../model/sound_emotion_model_info.txt', 'w') as f:\n",
    "    f.write(f\"Sound Emotion Detection Model\\n\")\n",
    "    f.write(f\"Input shape: {input_shape}\\n\")\n",
    "    f.write(f\"Number of classes: {n_classes}\\n\")\n",
    "    f.write(f\"Classes: {list(class_names)}\\n\")\n",
    "    f.write(f\"Sample rate: {sample_rate}\\n\")\n",
    "    f.write(f\"Duration: {duration} seconds\\n\")\n",
    "    f.write(f\"MFCC features: {n_mfcc}\\n\")\n",
    "\n",
    "print(\"âœ… Model information saved\")\n",
    "print(\"\\nðŸŽ‰ Sound Emotion Detection Model Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model on a new audio file\n",
    "def predict_sound_emotion(audio_path, model, scaler, label_encoder):\n",
    "    \"\"\"\n",
    "    Predict emotion from audio file\n",
    "    \"\"\"\n",
    "    # Extract features\n",
    "    features = extract_audio_features(audio_path)\n",
    "    \n",
    "    if features is None:\n",
    "        return None\n",
    "    \n",
    "    # Combine features\n",
    "    combined_features = np.concatenate([\n",
    "        features['mfcc'],\n",
    "        features['chroma'],\n",
    "        features['spectral_contrast'],\n",
    "        features['zcr'],\n",
    "        features['spectral_rolloff'],\n",
    "        features['rms']\n",
    "    ], axis=0)\n",
    "    \n",
    "    # Normalize\n",
    "    normalized_features = scaler.transform([combined_features])\n",
    "    \n",
    "    # Reshape\n",
    "    reshaped_features = normalized_features.reshape(1, n_time_steps, n_features_per_step)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(reshaped_features, verbose=0)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
    "    confidence = np.max(prediction)\n",
    "    \n",
    "    # Decode label\n",
    "    predicted_emotion = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "    return {\n",
    "        'emotion': predicted_emotion,\n",
    "        'confidence': float(confidence),\n",
    "        'all_probabilities': {\n",
    "            label_encoder.inverse_transform([i])[0]: float(prob) \n",
    "            for i, prob in enumerate(prediction[0])\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test on a sample file\n",
    "if len(file_paths) > 0:\n",
    "    test_file = file_paths[0]\n",
    "    print(f\"\\nTesting on: {test_file}\")\n",
    "    \n",
    "    prediction = predict_sound_emotion(test_file, model, scaler, label_encoder)\n",
    "    \n",
    "    if prediction:\n",
    "        print(f\"Predicted emotion: {prediction['emotion']}\")\n",
    "        print(f\"Confidence: {prediction['confidence']:.4f}\")\n",
    "        print(f\"\\nAll probabilities:\")\n",
    "        for emotion, prob in prediction['all_probabilities'].items():\n",
    "            print(f\"  {emotion}: {prob:.4f}\")\n",
    "    else:\n",
    "        print(\"Failed to predict emotion\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
